#!/usr/bin/env python3
"""
Conformal Prediction for Drift Detection in Numerários Model Predictions

This script implements conformal prediction methods to detect drift in model predictions
by comparing predicted values with actual values over time. It uses p-values and 
martingales to identify when the model's predictions become unreliable.

Theory: Conformal prediction provides distribution-free uncertainty quantification
under the exchangeability assumption. When this assumption is violated (drift occurs),
p-values are no longer uniformly distributed, which can be detected using martingales.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Tuple, List, Dict, Optional, Union
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# =============================================================================
# 1. DATA LOADING AND PREPROCESSING
# =============================================================================

def load_and_merge_data(predictions_path: str, features_path: str) -> pd.DataFrame:
    """
    Load predictions and real values data, then merge them.
    
    Args:
        predictions_path: Path to CSV file with predictions
        features_path: Path to parquet file with real values
    
    Returns:
        Merged DataFrame with predictions and real values
    """
    print("Loading data files...")
    
    # Load predictions from CSV
    predictions_df = pd.read_csv(predictions_path)
    print(f"Loaded predictions: {predictions_df.shape}")
    
    # Load features/real values from parquet
    features_df = pd.read_parquet(features_path)
    print(f"Loaded features: {features_df.shape}")
    
    # Convert DATA column to datetime in both dataframes
    predictions_df['DATA'] = pd.to_datetime(predictions_df['DATA'])
    features_df['DATA'] = pd.to_datetime(features_df['DATA'])
    
    # Merge on AGENCIA and DATA
    merged_df = pd.merge(
        predictions_df,
        features_df,
        on=['AGENCIA', 'DATA'],
        how='inner',
        suffixes=('_pred', '_real')
    )
    
    print(f"Merged data shape: {merged_df.shape}")
    print(f"Agencies in data: {merged_df['AGENCIA'].nunique()}")
    
    return merged_df

# =============================================================================
# 2. NONCONFORMITY SCORE FUNCTIONS
# =============================================================================

def absolute_residual_score(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
    """
    Calculate absolute residual nonconformity scores.
    
    Theory: The nonconformity score measures how different a prediction is from
    the true value. Under exchangeability, these scores should be exchangeable.
    
    Args:
        y_true: True values
        y_pred: Predicted values
    
    Returns:
        Array of nonconformity scores
    """
    return np.abs(y_true - y_pred)

def normalized_residual_score(y_true: np.ndarray, y_pred: np.ndarray, 
                            epsilon: float = 1e-8) -> np.ndarray:
    """
    Calculate normalized residual nonconformity scores.
    
    Theory: Normalization can help handle heteroscedasticity by scaling residuals
    relative to the prediction magnitude.
    
    Args:
        y_true: True values
        y_pred: Predicted values
        epsilon: Small constant to avoid division by zero
    
    Returns:
        Array of normalized nonconformity scores
    """
    return np.abs(y_true - y_pred) / (np.abs(y_pred) + epsilon)

def signed_residual_score(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:
    """
    Calculate signed residual nonconformity scores.
    
    Theory: Signed residuals can detect directional drift (systematic over/under-prediction).
    
    Args:
        y_true: True values
        y_pred: Predicted values
    
    Returns:
        Array of signed nonconformity scores
    """
    return y_true - y_pred

# =============================================================================
# 3. P-VALUE COMPUTATION
# =============================================================================

def compute_conformal_pvalue(calibration_scores: np.ndarray, 
                           test_score: float,
                           tie_breaking: str = 'random') -> float:
    """
    Compute conformal p-value for a test observation.
    
    Theory: The p-value represents the proportion of calibration scores that are
    at least as extreme as the test score. Under exchangeability, p-values are
    uniformly distributed on [0,1].
    
    Args:
        calibration_scores: Array of calibration nonconformity scores
        test_score: Nonconformity score for test observation
        tie_breaking: Method for handling ties ('random' or 'conservative')
    
    Returns:
        Conformal p-value
    """
    n = len(calibration_scores)
    
    if tie_breaking == 'random':
        # Random tie-breaking for exact validity
        theta = np.random.uniform(0, 1)
        rank = np.sum(calibration_scores < test_score) + \
               theta * np.sum(calibration_scores == test_score)
    else:
        # Conservative approach
        rank = np.sum(calibration_scores <= test_score)
    
    # Add 1 to include the test observation itself
    p_value = (rank + 1) / (n + 1)
    
    return p_value

def compute_pvalues_batch(calibration_scores: np.ndarray,
                         test_scores: np.ndarray) -> np.ndarray:
    """
    Compute p-values for multiple test observations efficiently.
    
    Args:
        calibration_scores: Array of calibration nonconformity scores
        test_scores: Array of test nonconformity scores
    
    Returns:
        Array of p-values
    """
    p_values = []
    for score in test_scores:
        p_value = compute_conformal_pvalue(calibration_scores, score)
        p_values.append(p_value)
    
    return np.array(p_values)

# =============================================================================
# 4. MARTINGALE FUNCTIONS
# =============================================================================

class SimpleMartingale:
    """
    Simple betting martingale for drift detection.
    
    Theory: Martingales transform p-values into a process that grows exponentially
    when drift occurs while remaining bounded under the null hypothesis.
    """
    
    def __init__(self):
        self.value = 1.0
        self.history = [1.0]
    
    def update(self, p_value: float) -> float:
        """Update martingale with new p-value using simple betting function."""
        # Simple betting function: bet more on small p-values
        bet = 1.5 if p_value < 0.5 else 0.5
        self.value *= bet
        self.history.append(self.value)
        return self.value
    
    def reset(self):
        """Reset martingale to initial state."""
        self.value = 1.0
        self.history = [1.0]

class MixtureMartingale:
    """
    Mixture martingale using logarithmic betting function.
    
    Theory: The mixture martingale integrates over all possible betting strategies,
    providing good power against various types of drift.
    """
    
    def __init__(self):
        self.value = 1.0
        self.history = [1.0]
    
    def update(self, p_value: float) -> float:
        """Update martingale with new p-value using mixture betting function."""
        # Mixture betting function approximation
        if p_value > 1e-10:
            bet = -1 / np.log(p_value)
        else:
            bet = 1e10  # Cap for numerical stability
        
        self.value *= bet
        self.history.append(self.value)
        return self.value
    
    def reset(self):
        """Reset martingale to initial state."""
        self.value = 1.0
        self.history = [1.0]

class PowerMartingale:
    """
    Power martingale with configurable parameter.
    
    Theory: Power martingales use the betting function g(p) = ε·p^(ε-1),
    where ε controls the sensitivity to different p-value ranges.
    """
    
    def __init__(self, epsilon: float = 0.5):
        self.epsilon = epsilon
        self.value = 1.0
        self.history = [1.0]
    
    def update(self, p_value: float) -> float:
        """Update martingale with new p-value using power betting function."""
        if p_value > 1e-10:
            bet = self.epsilon * (p_value ** (self.epsilon - 1))
        else:
            bet = 1e10  # Cap for numerical stability
            
        self.value *= bet
        self.history.append(self.value)
        return self.value
    
    def reset(self):
        """Reset martingale to initial state."""
        self.value = 1.0
        self.history = [1.0]

# =============================================================================
# 5. DRIFT DETECTION PROCEDURES
# =============================================================================

def ville_procedure(martingale_values: List[float], threshold: float = 100) -> Optional[int]:
    """
    Ville's inequality-based drift detection.
    
    Theory: By Ville's inequality, P(max(S_n) >= C) <= 1/C for any martingale S_n. 
    This provides false alarm control at level 1/threshold.
    
    Args:
        martingale_values: List of martingale values
        threshold: Detection threshold
    
    Returns:
        Index of first drift detection or None
    """
    for i, value in enumerate(martingale_values):
        if value >= threshold:
            return i
    return None

def cusum_procedure(martingale_values: List[float], threshold: float = 20) -> Optional[int]:
    """
    CUSUM-based change detection using martingale ratios.
    
    Theory: CUSUM detects persistent changes by accumulating evidence over time.
    
    Args:
        martingale_values: List of martingale values
        threshold: Detection threshold for CUSUM statistic
    
    Returns:
        Index of first drift detection or None
    """
    for i in range(1, len(martingale_values)):
        # Compute maximum ratio S_n / S_j for all j <= n
        max_ratio = max(martingale_values[i] / martingale_values[j] 
                       for j in range(i + 1))
        if max_ratio >= threshold:
            return i
    return None

# =============================================================================
# 6. CONFORMAL DRIFT DETECTOR CLASS
# =============================================================================

class ConformalDriftDetector:
    """
    Main class for conformal prediction-based drift detection.
    
    This class implements the complete workflow for detecting drift in predictions
    using conformal prediction, p-values, and martingales.
    """
    
    def __init__(self, 
                 nonconformity_measure: str = 'absolute_residual',
                 martingale_type: str = 'simple',
                 detection_threshold: float = 100,
                 calibration_window: int = 500):
        """
        Initialize drift detector.
        
        Args:
            nonconformity_measure: Type of nonconformity score to use
            martingale_type: Type of martingale ('simple', 'mixture', 'power')
            detection_threshold: Threshold for drift detection
            calibration_window: Size of calibration window
        """
        self.nonconformity_measure = nonconformity_measure
        self.detection_threshold = detection_threshold
        self.calibration_window = calibration_window
        
        # Initialize martingale
        if martingale_type == 'simple':
            self.martingale = SimpleMartingale()
        elif martingale_type == 'mixture':
            self.martingale = MixtureMartingale()
        elif martingale_type == 'power':
            self.martingale = PowerMartingale()
        else:
            raise ValueError(f"Unknown martingale type: {martingale_type}")
        
        # Storage for results
        self.calibration_scores = []
        self.p_values = []
        self.timestamps = []
        self.drift_points = []
        
    def compute_nonconformity_score(self, y_true: float, y_pred: float) -> float:
        """Compute nonconformity score based on selected measure."""
        if self.nonconformity_measure == 'absolute_residual':
            return absolute_residual_score(np.array([y_true]), np.array([y_pred]))[0]
        elif self.nonconformity_measure == 'normalized_residual':
            return normalized_residual_score(np.array([y_true]), np.array([y_pred]))[0]
        elif self.nonconformity_measure == 'signed_residual':
            return signed_residual_score(np.array([y_true]), np.array([y_pred]))[0]
        else:
            raise ValueError(f"Unknown nonconformity measure: {self.nonconformity_measure}")
    
    def process_data_stream(self, data: pd.DataFrame) -> Dict:
        """
        Process a stream of data for drift detection.
        
        Args:
            data: DataFrame with columns including predictions and real values
        
        Returns:
            Dictionary with detection results
        """
        # Sort by date
        data = data.sort_values('DATA').reset_index(drop=True)
        
        # Initialize with calibration period
        for i in range(self.calibration_window):
            score = self.compute_nonconformity_score(
                data.iloc[i]['VALOR_real'],
                data.iloc[i]['VALOR_pred']
            )
            self.calibration_scores.append(score)
        
        # Process remaining data
        for i in range(self.calibration_window, len(data)):
            # Compute nonconformity score for new observation
            score = self.compute_nonconformity_score(
                data.iloc[i]['VALOR_real'],
                data.iloc[i]['VALOR_pred']
            )
            
            # Calculate p-value
            p_value = compute_conformal_pvalue(
                np.array(self.calibration_scores[-self.calibration_window:]),
                score
            )
            
            # Update martingale
            martingale_value = self.martingale.update(p_value)
            
            # Store results
            self.p_values.append(p_value)
            self.timestamps.append(data.iloc[i]['DATA'])
            
            # Check for drift
            if martingale_value >= self.detection_threshold:
                self.drift_points.append(i)
                # Optionally reset martingale after detection
                # self.martingale.reset()
            
            # Update calibration window (sliding window approach)
            self.calibration_scores.append(score)
            if len(self.calibration_scores) > self.calibration_window * 2:
                self.calibration_scores.pop(0)
        
        return {
            'p_values': self.p_values,
            'martingale_history': self.martingale.history[1:],  # Exclude initial 1.0
            'timestamps': self.timestamps,
            'drift_points': self.drift_points
        }

# =============================================================================
# 7. VISUALIZATION FUNCTIONS
# =============================================================================

def plot_drift_detection_results(results: Dict, agency_name: str):
    """
    Create comprehensive visualization of drift detection results.
    
    Args:
        results: Dictionary with detection results
        agency_name: Name of the agency being analyzed
    """
    fig, axes = plt.subplots(3, 1, figsize=(14, 12))
    
    # 1. P-values over time
    ax1 = axes[0]
    ax1.plot(results['timestamps'], results['p_values'], 'b-', alpha=0.7, linewidth=1)
    ax1.axhline(y=0.05, color='r', linestyle='--', label='α = 0.05')
    ax1.fill_between(results['timestamps'], 0, 0.05, alpha=0.2, color='red')
    
    # Mark drift points
    for drift_idx in results['drift_points']:
        if drift_idx - len(results['timestamps']) + len(results['p_values']) >= 0:
            adj_idx = drift_idx - len(results['timestamps']) + len(results['p_values'])
            ax1.axvline(x=results['timestamps'][adj_idx], color='red', 
                       linestyle=':', alpha=0.7)
    
    ax1.set_ylabel('P-value', fontsize=12)
    ax1.set_title(f'Conformal P-values Over Time - Agency: {agency_name}', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Martingale evolution (log scale)
    ax2 = axes[1]
    ax2.semilogy(results['timestamps'], results['martingale_history'], 
                 'g-', linewidth=2, label='Martingale Value')
    ax2.axhline(y=results.get('threshold', 100), color='r', linestyle='--', 
                label='Detection Threshold')
    
    # Mark drift points
    for drift_idx in results['drift_points']:
        if drift_idx - len(results['timestamps']) + len(results['p_values']) >= 0:
            adj_idx = drift_idx - len(results['timestamps']) + len(results['p_values'])
            ax2.axvline(x=results['timestamps'][adj_idx], color='red', 
                       linestyle=':', alpha=0.7)
    
    ax2.set_ylabel('Martingale Value (log scale)', fontsize=12)
    ax2.set_title('Martingale Evolution for Drift Detection', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. P-value distribution histogram
    ax3 = axes[2]
    ax3.hist(results['p_values'], bins=30, alpha=0.7, edgecolor='black', density=True)
    ax3.axvline(x=0.05, color='r', linestyle='--', label='α = 0.05')
    
    # Add uniform distribution reference
    x_uniform = np.linspace(0, 1, 100)
    ax3.plot(x_uniform, np.ones_like(x_uniform), 'k--', alpha=0.5, 
             label='Uniform (no drift)')
    
    ax3.set_xlabel('P-value', fontsize=12)
    ax3.set_ylabel('Density', fontsize=12)
    ax3.set_title('P-value Distribution (Should be Uniform Under No Drift)', fontsize=14)
    ax3.legend()
    
    plt.tight_layout()
    plt.savefig(f'drift_detection_{agency_name}.png', dpi=300, bbox_inches='tight')
    plt.show()

def plot_prediction_analysis(data: pd.DataFrame, agency_name: str, 
                           drift_points: List[int] = None):
    """
    Plot predictions vs real values with drift points marked.
    
    Args:
        data: DataFrame with predictions and real values
        agency_name: Name of the agency
        drift_points: List of indices where drift was detected
    """
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))
    
    # 1. Time series of predictions and real values
    ax1.plot(data['DATA'], data['VALOR_real'], 'b-', label='Real Values', alpha=0.8)
    ax1.plot(data['DATA'], data['VALOR_pred'], 'r--', label='Predictions', alpha=0.8)
    
    # Mark drift points
    if drift_points:
        for drift_idx in drift_points:
            if drift_idx < len(data):
                ax1.axvline(x=data.iloc[drift_idx]['DATA'], color='orange', 
                           linestyle=':', alpha=0.7, label='Drift Detected' if drift_idx == drift_points[0] else '')
    
    ax1.set_ylabel('Value', fontsize=12)
    ax1.set_title(f'Predictions vs Real Values - Agency: {agency_name}', fontsize=14)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Residuals over time
    residuals = data['VALOR_real'] - data['VALOR_pred']
    ax2.scatter(data['DATA'], residuals, alpha=0.5, s=20)
    ax2.axhline(y=0, color='k', linestyle='-', alpha=0.5)
    
    # Add rolling average of residuals
    rolling_mean = residuals.rolling(window=30, min_periods=1).mean()
    ax2.plot(data['DATA'], rolling_mean, 'r-', linewidth=2, label='30-day Rolling Mean')
    
    ax2.set_xlabel('Date', fontsize=12)
    ax2.set_ylabel('Residual (Real - Predicted)', fontsize=12)
    ax2.set_title('Prediction Residuals Over Time', fontsize=14)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'prediction_analysis_{agency_name}.png', dpi=300, bbox_inches='tight')
    plt.show()

# =============================================================================
# 8. MAIN ANALYSIS FUNCTION
# =============================================================================

def analyze_agency_drift(data: pd.DataFrame, agency_id: str, 
                        martingale_type: str = 'simple',
                        threshold: float = 100) -> Dict:
    """
    Perform complete drift analysis for a specific agency.
    
    Args:
        data: Merged DataFrame with predictions and real values
        agency_id: Agency identifier to analyze
        martingale_type: Type of martingale to use
        threshold: Detection threshold
    
    Returns:
        Dictionary with analysis results
    """
    # Filter data for specific agency
    agency_data = data[data['AGENCIA'] == agency_id].copy()
    
    if len(agency_data) < 100:
        print(f"Warning: Agency {agency_id} has only {len(agency_data)} observations.")
        return None
    
    print(f"\nAnalyzing Agency: {agency_id}")
    print(f"Data points: {len(agency_data)}")
    print(f"Date range: {agency_data['DATA'].min()} to {agency_data['DATA'].max()}")
    
    # Initialize drift detector
    detector = ConformalDriftDetector(
        nonconformity_measure='absolute_residual',
        martingale_type=martingale_type,
        detection_threshold=threshold,
        calibration_window=min(500, len(agency_data) // 3)
    )
    
    # Process data stream
    results = detector.process_data_stream(agency_data)
    results['threshold'] = threshold
    
    # Calculate drift statistics
    n_drifts = len(results['drift_points'])
    drift_rate = n_drifts / len(results['p_values']) if results['p_values'] else 0
    
    print(f"\nDrift Detection Results:")
    print(f"- Number of drift points detected: {n_drifts}")
    print(f"- Drift rate: {drift_rate:.2%}")
    print(f"- Average p-value: {np.mean(results['p_values']):.3f}")
    print(f"- Max martingale value: {max(results['martingale_history']):.2f}")
    
    # Create visualizations
    plot_drift_detection_results(results, agency_id)
    plot_prediction_analysis(agency_data, agency_id, results['drift_points'])
    
    return results

# =============================================================================
# 9. MAIN EXECUTION
# =============================================================================

def main():
    """Main execution function."""
    
    # File paths
    predictions_path = "previsoes_numerario_pre_pos_pandemia.csv"
    features_path = "features_numerario.parquet"
    
    try:
        # Load and merge data
        merged_data = load_and_merge_data(predictions_path, features_path)
        
        # Select an agency for detailed analysis
        # You can modify this to analyze multiple agencies
        agencies = merged_data['AGENCIA'].unique()
        print(f"\nAvailable agencies: {len(agencies)}")
        print(f"First 10 agencies: {agencies[:10]}")
        
        # Analyze the agency with most data points
        agency_counts = merged_data['AGENCIA'].value_counts()
        selected_agency = agency_counts.index[0]
        
        print(f"\nSelected agency for analysis: {selected_agency}")
        print(f"Number of observations: {agency_counts.iloc[0]}")
        
        # Perform drift analysis
        results = analyze_agency_drift(
            merged_data,
            selected_agency,
            martingale_type='simple',
            threshold=100
        )
        
        # Optionally analyze multiple agencies
        print("\n" + "="*60)
        print("SUMMARY OF TOP 5 AGENCIES")
        print("="*60)
        
        summary_results = []
        for agency in agency_counts.index[:5]:
            agency_data = merged_data[merged_data['AGENCIA'] == agency]
            if len(agency_data) >= 100:
                detector = ConformalDriftDetector()
                results = detector.process_data_stream(agency_data)
                
                summary_results.append({
                    'agency': agency,
                    'n_observations': len(agency_data),
                    'n_drifts': len(results['drift_points']),
                    'drift_rate': len(results['drift_points']) / len(results['p_values']),
                    'avg_p_value': np.mean(results['p_values'])
                })
        
        # Display summary
        summary_df = pd.DataFrame(summary_results)
        print(summary_df.to_string(index=False))
        
    except FileNotFoundError as e:
        print(f"Error: Could not find data file - {e}")
        print("Please ensure the following files are in the current directory:")
        print("- previsoes_numerario_pre_pos_pandemia.csv")
        print("- features_numerario.parquet")
    except Exception as e:
        print(f"Error during analysis: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
